{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import clone\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from helper_functions import *\n",
    "\n",
    "# Load all pre-processed data sets if available.\n",
    "data_list = get_data_list(divs=[4,10,20])\n",
    "\n",
    "# Parameters\n",
    "n_classes = 5\n",
    "n_estimators = 15\n",
    "RANDOM_SEED = 42  # fix the seed on each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(5),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    SVC(gamma=2, C=1,degree=4),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
    "    DecisionTreeClassifier(max_depth=None),\n",
    "    RandomForestClassifier(max_depth=None, n_estimators=n_estimators, max_features=int(np.sqrt(n_estimators))),\n",
    "    MLPClassifier(),\n",
    "    MLPClassifier(hidden_layer_sizes=(10,10)),\n",
    "    MLPClassifier(hidden_layer_sizes=(10,10,10)),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "divs=[4,10,20]\n",
    "combis = get_combis(divs)\n",
    "\n",
    "for index, version in enumerate(data_list):\n",
    "    feature_size = combis[index]\n",
    "    X = version.drop(['Target','label'],axis=1).values\n",
    "    y = version['Target']\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "#         if hasattr(clf, \"decision_function\"):\n",
    "#             Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "#         else:\n",
    "#             Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        print(\"{} with features {} has a score of {}\".format(name,feature_size,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [DecisionTreeClassifier(max_depth=None),\n",
    "          RandomForestClassifier(n_estimators=n_estimators),\n",
    "          ExtraTreesClassifier(n_estimators=n_estimators),\n",
    "          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                             n_estimators=n_estimators)]\n",
    "for version in data_list:\n",
    "    feature_size = combis[index]\n",
    "    X = version.drop(['Target','label'],axis=1).values\n",
    "    y = version['Target']\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    # iterate over classifiers\n",
    "\n",
    "    for model in models:\n",
    "            # Train models\n",
    "            clf = clone(model)\n",
    "            clf = model.fit(X_train, y_train)\n",
    "\n",
    "            scores = clf.score(X_test, y_test)\n",
    "            # Create a title for each column and the console by using str() and\n",
    "            # slicing away useless parts of the string\n",
    "            model_title = str(type(model)).split(\".\")[-1][:-2][:-len(\"Classifier\")]\n",
    "            model_details = model_title\n",
    "            if hasattr(model, \"estimators_\"):\n",
    "                model_details += \" with {} estimators\".format(len(model.estimators_))\n",
    "            print( model_details + \" with features\", feature_size, \"has a score of\", scores )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cvloop]",
   "language": "python",
   "name": "conda-env-cvloop-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
